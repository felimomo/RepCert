Here I just keep track of some single-shot runs of test.py I've done. For each 
group, noise level and each threshold prob of false-positive, I will write a different
data stack. Each data stack contains the total dimension, the computation time, and
whether or not the irreducibility test was passed. The data stack is ordered with
ascending dimension.

As expected, the time complexity is driven by the number of samples needed, which
itself depends delicately on the dimension of the irrep being tested. 

For these samples I used extremely short random walks, starting with the cayley graph
diameter, and ending at that number +100.

1dim irreps are trivial to check, so I leave them out of the 
analysis. The computations are don in institute desktops with either i5 or i7
processors of 6700K or 8400K. I don't keep track of where each specific run is
run since I'm just interested in an estimate for the time required in typical
university conditions.

----------------------------------------------------------------
Group = s5 |--| Noise level = 10^-12 |--| Threshold prob = 10^-7
----------------------------------------------------------------

Irrep dimensions: 4, 5, 6

Testing a 6-dim subrep is associated to 18k samples, testing a
5-dim subrep is associated to 12k samples, testing a 4-dim 
subrep is associated to 8252 samples, and testing a 1-dim
subrep is trivial.

Cert 	Dim	irr	samples		Run time
-------------------------------------------------------
Yes	410	5	12894		589s   (10 min)
Yes	509	4	8252		600s   (10 min)
Yes	723	5	12894		2438s  (40 min)
Yes	736	5	12894		2699s  (45 min)

